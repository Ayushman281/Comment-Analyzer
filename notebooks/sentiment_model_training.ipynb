{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports and Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe kernel failed to start due to the missing module 'IPython.core.application'. Consider installing this module.\n",
      "\u001b[1;31mClick <a href='https://aka.ms/kernelFailuresMissingModule'>here</a> for more info."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import emoji\n",
    "import contractions\n",
    "from langdetect import detect\n",
    "from googletrans import Translator\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import (\n",
    "    BertTokenizerFast,\n",
    "    BertForSequenceClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    AutoTokenizer\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import Dataset  \n",
    "\n",
    "os.environ[\"WANDB_MODE\"] = \"disabled\"\n",
    "\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "translator = Translator()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cleaning Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_comment(text):\n",
    "    # Lowercase text\n",
    "    text = text.lower()\n",
    "    # Remove URLs\n",
    "    text = re.sub(r'http\\S+|www.\\S+', '', text)\n",
    "    # Replace mentions with <USER>\n",
    "    text = re.sub(r'@\\w+', '<USER>', text)\n",
    "    # Convert emojis to text\n",
    "    text = emoji.demojize(text)\n",
    "    # Expand contractions\n",
    "    text = contractions.fix(text)\n",
    "    # Reduce repeated characters\n",
    "    text = re.sub(r'(.)\\1{2,}', r'\\1', text)\n",
    "    # Remove special characters and numbers (keep alphabets and spaces)\n",
    "    text = re.sub(r'[^a-z\\s]', '', text)\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    # Remove stopwords\n",
    "    text = ' '.join(word for word in text.split() if word not in stop_words)\n",
    "    # Remove timestamps and promotional phrases\n",
    "    text = re.sub(r'\\b\\d{1,2}:\\d{2}\\b', '', text)\n",
    "    text = re.sub(r'(subscribe|check out my channel)', '', text)\n",
    "    # Translate non-English text if necessary\n",
    "    try:\n",
    "        if detect(text) != 'en':\n",
    "            text = translator.translate(text, dest='en').text\n",
    "    except Exception as e:\n",
    "        pass\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loading and Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = r'D:\\College\\Projects\\Comment Analyzer\\data\\youtube_comments_cleaned.csv'\n",
    "df = pd.read_csv(csv_path)\n",
    "\n",
    "df = df[['VideoID', 'CommentText', 'Sentiment', 'Likes', 'Replies']]\n",
    "\n",
    "df['Likes'] = pd.to_numeric(df['Likes'], errors='coerce').fillna(0)\n",
    "df['Replies'] = pd.to_numeric(df['Replies'], errors='coerce').fillna(0)\n",
    "\n",
    "print(df.head())\n",
    "\n",
    "alpha = 1.0\n",
    "beta = 0.5\n",
    "\n",
    "df['sample_weight'] = (alpha * df['Likes']) + (beta * df['Replies']) + 1\n",
    "df['cleaned_comment'] = df['CommentText'].apply(clean_comment)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Custom PyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2,\n",
    "    'Neutral': 1,\n",
    "    'Positive': 2,\n",
    "    'Negative': 0\n",
    "}\n",
    "\n",
    "tokenizer = BertTokenizerFast.from_pretrained('bert-base-uncased')\n",
    "\n",
    "class YouTubeCommentsDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_length=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        row = self.dataframe.iloc[index]\n",
    "        encoding = self.tokenizer(\n",
    "            row['cleaned_comment'],\n",
    "            truncation=True,\n",
    "            padding='max_length',\n",
    "            max_length=self.max_length,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        item = {key: val.squeeze() for key, val in encoding.items()}\n",
    "        item['labels'] = torch.tensor(label2id[row['Sentiment']])\n",
    "        item['sample_weight'] = torch.tensor(row['sample_weight'], dtype=torch.float)\n",
    "        return item\n",
    "\n",
    "# Create an instance of the custom dataset\n",
    "custom_dataset = YouTubeCommentsDataset(df, tokenizer)\n",
    "\n",
    "# Print the first 5 samples from the custom dataset\n",
    "for i in range(5):\n",
    "    print(f\"Custom Dataset Sample {i}:\", custom_dataset[i])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Mapping for Hugging Face Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label2id = {\n",
    "    'negative': 0,\n",
    "    'neutral': 1,\n",
    "    'positive': 2,\n",
    "    'Neutral': 1,\n",
    "    'Positive': 2,\n",
    "    'Negative': 0\n",
    "}\n",
    "\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "def create_input_features(example):\n",
    "    encodings = tokenizer(example['CommentText'], padding='max_length', truncation=True, max_length=128)\n",
    "    encodings['labels'] = label2id[example['Sentiment'].lower()]\n",
    "    encodings['sample_weight'] = example['Weightage'] \n",
    "    return encodings\n",
    "\n",
    "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_df['Weightage'] = (alpha * train_df['Likes']) + (beta * train_df['Replies']) + 1\n",
    "eval_df['Weightage'] = (alpha * eval_df['Likes']) + (beta * eval_df['Replies']) + 1\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_df)\n",
    "eval_dataset = Dataset.from_pandas(eval_df)\n",
    "\n",
    "train_dataset = train_dataset.map(create_input_features)\n",
    "eval_dataset = eval_dataset.map(create_input_features)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the Custom Model with Weighted Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WeightedBertForSequenceClassification(BertForSequenceClassification):\n",
    "    def compute_loss(self, model, inputs, return_outputs=False):\n",
    "        labels = inputs.pop(\"labels\")\n",
    "        sample_weights = inputs.pop(\"sample_weight\")\n",
    "        outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        loss = F.cross_entropy(logits, labels, reduction='none')\n",
    "        weighted_loss = (loss * sample_weights).mean()\n",
    "        return (weighted_loss, outputs) if return_outputs else weighted_loss\n",
    "\n",
    "model = WeightedBertForSequenceClassification.from_pretrained(model_name, num_labels=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set Up Training and Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=8,\n",
    "    per_device_train_batch_size=16,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1)\n",
    "    accuracy = (predictions == labels).mean()\n",
    "    return {'accuracy': accuracy}\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train, Save, and Use the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.train()\n",
    "\n",
    "model_save_path = \"/content/fine_tuned_bert_model\"\n",
    "model.save_pretrained(model_save_path)\n",
    "tokenizer.save_pretrained(model_save_path)\n",
    "\n",
    "def predict_sentiment(comment, likes=0, replies=0, alpha=1.0, beta=0.5):\n",
    "    clean_text = clean_comment(comment)\n",
    "    inputs = tokenizer(clean_text, return_tensors=\"pt\", truncation=True, max_length=128, padding='max_length')\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    probabilities = torch.nn.functional.softmax(outputs.logits, dim=-1)\n",
    "    weight = (alpha * likes) + (beta * replies) + 1\n",
    "    weighted_probabilities = probabilities * weight\n",
    "    predicted_class = weighted_probabilities.argmax().item()\n",
    "    return predicted_class, probabilities\n",
    "\n",
    "#Test Case\n",
    "test_comment = \"This video is amazing! üòç\"\n",
    "pred_class, probs = predict_sentiment(test_comment, likes=10, replies=2)\n",
    "print(\"Predicted sentiment class:\", pred_class)\n",
    "print(\"Probabilities:\", probs)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
